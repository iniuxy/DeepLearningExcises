{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\" A clean, no_frills character-level generative language model.\n",
    "Created by Danijar Hafner (danijar.com), edited by Chip Huyen\n",
    "for the class CS 20SI: \"TensorFlow for Deep Learning Research\"\n",
    "\n",
    "Based on Andrej Karpathy's blog: \n",
    "http://karpathy.github.io/2015/05/21/rnn-effectiveness/\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import time\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "import examples.utils\n",
    "\n",
    "DATA_PATH = 'examples/data/arvix_abstracts.txt'\n",
    "HIDDEN_SIZE = 200\n",
    "BATCH_SIZE = 64\n",
    "NUM_STEPS = 50\n",
    "SKIP_STEP = 40\n",
    "TEMPRATURE = 0.7\n",
    "LR = 0.003\n",
    "LEN_GENERATED = 300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vocab_encode(text, vocab):\n",
    "    return [vocab.index(x) + 1 for x in text if x in vocab]\n",
    "\n",
    "def vocab_decode(array, vocab):\n",
    "    return ''.join([vocab[x - 1] for x in array])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_data(filename, vocab, window=NUM_STEPS, overlap=NUM_STEPS//2):\n",
    "    for text in open(filename):\n",
    "        text = vocab_encode(text, vocab)\n",
    "        for start in range(0, len(text) - window, overlap):\n",
    "            chunk = text[start: start + window]\n",
    "            chunk += [0] * (window - len(chunk))\n",
    "            yield chunk\n",
    "\n",
    "def read_batch(stream, batch_size=BATCH_SIZE):\n",
    "    batch = []\n",
    "    for element in stream:\n",
    "        batch.append(element)\n",
    "        if len(batch) == batch_size:\n",
    "            yield batch\n",
    "            batch = []\n",
    "    yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dynamic_rnn(\n",
    "    cell,\n",
    "    inputs,\n",
    "    sequence_length=None,\n",
    "    initial_state=None,\n",
    "    dtype=None,\n",
    "    parallel_iterations=None,\n",
    "    swap_memory=False,\n",
    "    time_major=False,\n",
    "    scope=None\n",
    ")\n",
    "\n",
    "A pair (outputs, state) where:\n",
    "\n",
    "outputs: The RNN output `Tensor`.\n",
    "\n",
    "  If time_major == False (default), this will be a `Tensor` shaped:\n",
    "    `[batch_size, max_time, cell.output_size]`.\n",
    "\n",
    "  If time_major == True, this will be a `Tensor` shaped:\n",
    "    `[max_time, batch_size, cell.output_size]`.\n",
    "\n",
    "  Note, if `cell.output_size` is a (possibly nested) tuple of integers\n",
    "  or `TensorShape` objects, then `outputs` will be a tuple having the\n",
    "  same structure as `cell.output_size`, containing Tensors having shapes\n",
    "  corresponding to the shape data in `cell.output_size`.\n",
    "\n",
    "state: The final state.  If `cell.state_size` is an int, this\n",
    "  will be shaped `[batch_size, cell.state_size]`.  If it is a\n",
    "  `TensorShape`, this will be shaped `[batch_size] + cell.state_size`.\n",
    "  If it is a (possibly nested) tuple of ints or `TensorShape`, this will\n",
    "  be a tuple having the corresponding shapes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_rnn(seq, hidden_size=HIDDEN_SIZE):\n",
    "    cell = tf.contrib.rnn.GRUCell(hidden_size)\n",
    "    in_state = tf.placeholder_with_default(\n",
    "            cell.zero_state(tf.shape(seq)[0], tf.float32), [None, hidden_size])\n",
    "    # this line to calculate the real length of seq\n",
    "    # all seq are padded to be of the same length which is NUM_STEPS\n",
    "    length = tf.reduce_sum(tf.reduce_max(tf.sign(seq), 2), 1)\n",
    "    output, out_state = tf.nn.dynamic_rnn(cell, seq, length, in_state)\n",
    "    return output, in_state, out_state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "tf.one_hot that can convert a set of sparse labels to a dense one-hot representation. This is in addition to tf.nn.sparse_softmax_cross_entropy_with_logits, which can in some cases let you compute the cross entropy directly on the sparse labels instead of converting them to one-hot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_model(seq, temp, vocab, hidden=HIDDEN_SIZE):\n",
    "    seq = tf.one_hot(seq, len(vocab))\n",
    "    output, in_state, out_state = create_rnn(seq, hidden)\n",
    "    # fully_connected is syntactic sugar for tf.matmul(w, output) + b\n",
    "    # it will create w and b for us\n",
    "    logits = tf.contrib.layers.fully_connected(output, len(vocab), None)\n",
    "    loss = tf.reduce_sum(tf.nn.softmax_cross_entropy_with_logits(logits=logits[:, :-1], labels=seq[:, 1:]))\n",
    "    # sample the next character from Maxwell-Boltzmann Distribution with temperature temp\n",
    "    # it works equally well without tf.exp\n",
    "    sample = tf.multinomial(tf.exp(logits[:, -1] / temp), 1)[:, 0] \n",
    "    return loss, sample, in_state, out_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state):\n",
    "    saver = tf.train.Saver()\n",
    "    start = time.time()\n",
    "    with tf.Session() as sess:\n",
    "        writer = tf.summary.FileWriter('graphs/gist', sess.graph)\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        \n",
    "        ckpt = tf.train.get_checkpoint_state(os.path.dirname('checkpoints/arvix/checkpoint'))\n",
    "        if ckpt and ckpt.model_checkpoint_path:\n",
    "            saver.restore(sess, ckpt.model_checkpoint_path)\n",
    "        \n",
    "        iteration = global_step.eval()\n",
    "        for batch in read_batch(read_data(DATA_PATH, vocab)):\n",
    "            batch_loss, _ = sess.run([loss, optimizer], {seq: batch})\n",
    "            if (iteration + 1) % SKIP_STEP == 0:\n",
    "                print('Iter {}. \\n    Loss {}. Time {}'.format(iteration, batch_loss, time.time() - start))\n",
    "                online_inference(sess, vocab, seq, sample, temp, in_state, out_state)\n",
    "                start = time.time()\n",
    "                saver.save(sess, 'checkpoints/arvix/char-rnn', iteration)\n",
    "            iteration += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def online_inference(sess, vocab, seq, sample, temp, in_state, out_state, seed='T'):\n",
    "    \"\"\" Generate sequence one character at a time, based on the previous character\n",
    "    \"\"\"\n",
    "    sentence = seed\n",
    "    state = None\n",
    "    for _ in range(LEN_GENERATED):\n",
    "        batch = [vocab_encode(sentence[-1], vocab)]\n",
    "        feed = {seq: batch, temp: TEMPRATURE}\n",
    "        # for the first decoder step, the state is None\n",
    "        if state is not None:\n",
    "            feed.update({in_state: state})\n",
    "        index, state = sess.run([sample, out_state], feed)\n",
    "        sentence += vocab_decode(index, vocab)\n",
    "    print(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 39. \n",
      "    Loss 9528.40625. Time 6.846832752227783\n",
      "Tki                                                                                                                                                                                                                                                                                                          \n",
      "Iter 79. \n",
      "    Loss 8567.9150390625. Time 6.672576904296875\n",
      "TeL on ther on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on ther on \n",
      "Iter 119. \n",
      "    Loss 7860.015625. Time 6.746322393417358\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 159. \n",
      "    Loss 7533.1904296875. Time 6.668008089065552\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 199. \n",
      "    Loss 6984.927734375. Time 6.659334421157837\n",
      "The the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the the t\n",
      "Iter 239. \n",
      "    Loss 6673.8525390625. Time 6.6917548179626465\n",
      "The semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the semple the \n",
      "Iter 279. \n",
      "    Loss 6483.2529296875. Time 6.666428565979004\n",
      "The suching and of the convertion the converting arg of the converting arg of the conver ard of the conver ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of the ard of \n",
      "Iter 319. \n",
      "    Loss 6693.1875. Time 6.631777286529541\n",
      "The semple and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting argorsing and propesting \n",
      "Iter 359. \n",
      "    Loss 5871.8994140625. Time 6.603971481323242\n",
      "The and the a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a propose station a\n",
      "Iter 399. \n",
      "    Loss 5592.6435546875. Time 6.633457183837891\n",
      "The and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the represent and the r\n",
      "Iter 439. \n",
      "    Loss 5422.49462890625. Time 6.6251561641693115\n",
      "The and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the and the a\n",
      "Iter 479. \n",
      "    Loss 4796.3388671875. Time 6.5947654247283936\n",
      "The and the expericentional networks (DNNs) and the state of the and the expericentional networks (DNNs) and the state of the and the expericentional networks (DNNs) and the arsing the and the and the and the and the and the and the expericentional networks (DNNs) and the arsing the and the and the a\n",
      "Iter 519. \n",
      "    Loss 4998.32373046875. Time 6.649388313293457\n",
      "The and the computation of the network and the convexion of the network and the convexion of the network and the convexion of the network and the convexion of the network and the convexion of the network and the convexion of the network and the convexion of the network and the computation of the netw\n",
      "Iter 559. \n",
      "    Loss 5656.77392578125. Time 6.625085115432739\n",
      "The and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder and stalle the significantly encoder a\n",
      "Iter 599. \n",
      "    Loss 4810.80078125. Time 6.607611656188965\n",
      "The and expendent despent and a deep neural networks (DNN) ard regularizer and standard networks (DNN)) and the reconting the mathin the matring the reconting the mathin the matring and a deep neural networks (DNN)) and the reconting the mathin the matring the reconting the mathin the matring and a d\n",
      "Iter 639. \n",
      "    Loss 4240.755859375. Time 6.6211934089660645\n",
      "The a propose a network a context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the context of the conte\n",
      "Iter 679. \n",
      "    Loss 4384.2783203125. Time 6.638440370559692\n",
      "The and the reculrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the recurrent deep learning and the r\n",
      "Iter 719. \n",
      "    Loss 3842.725341796875. Time 6.5975306034088135\n",
      "The are introduce a contrain the propose a context of the aral are introduce a contrain the propose a context of the aral are introduce a computation of the are and are interented of the are and are interented of the are and are interented of the are and are interented of the are and are interented o\n",
      "Iter 759. \n",
      "    Loss 4367.3388671875. Time 6.603273868560791\n",
      "The and the provide the state-of-the-art neural networks are standard neural networks are standard neural networks are standard neural networks are standard neural networks are standard neural networks are standard neural networks are standard neural networks (DNN) are standard neural networks (DNN) \n",
      "Iter 799. \n",
      "    Loss 4225.8349609375. Time 6.604463577270508\n",
      "The also state-of-the-art propose a new a simple model as a simple model as a simple model as the state-of-the-art propose a new a simple model as a simple model as the state-of-the-art propose a new a simple model as a simple model as the state-of-the-art propose a new a simple model as a simple mod\n",
      "Iter 839. \n",
      "    Loss 3616.12890625. Time 6.632296800613403\n",
      "The and employ the network architectures that are in the convex optimization of the network destrap networks and an artient and computational neural networks and an artient and convex optimization and the convex optimization of the network deep neural networks and an artient and convex optimization a\n",
      "Iter 879. \n",
      "    Loss 3618.69873046875. Time 6.620789051055908\n",
      "The convergence of the stale-that the convex optimization are explicit of the and the different distribution and the different distribution and the different distribution and the different distribution and the different distribution and the different distribution and the different distribution and th\n",
      "Iter 919. \n",
      "    Loss 3688.63427734375. Time 6.637659072875977\n",
      "The and experiments of the architectures and a deep networks (DNNs) a formal structural for the architectures and a deep networks (DNNs) a formal structural for the architectures and a deep networks (DNNs) a formal structural for the architectures and a deep networks (DNNs) a formal structural for th\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 959. \n",
      "    Loss 3835.907958984375. Time 6.585753917694092\n",
      "The computation of the network convergence reconstrated to a convex optimization processing and the network deconstrated and the network deconstrated and the network deconstrated and the network deconstrated and the network deconstrated and the network deconstrated and the network decoder and standar\n",
      "Iter 999. \n",
      "    Loss 3426.80615234375. Time 6.586920499801636\n",
      "The and the simple experimental results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art results on a state-of-the-art\n",
      "Iter 1039. \n",
      "    Loss 3630.8857421875. Time 6.6226770877838135\n",
      "The algorithm for the results in search for the recurtent describution in the proposed deep neural networks. We ored to the results in searca essive the network properties the parameters the network are state-of-the-art performance of a set of deep networks to state of the results of the results of t\n",
      "Iter 1079. \n",
      "    Loss 3368.67529296875. Time 6.648385286331177\n",
      "The convergence reconstruction of the input of the and stacked to be applied to a single learning processing are and stacked to be an an experimental on improvement and deep neural networks (DNN) howe provide a new an a training of the and stacked to be an an experimental on improvements in a recense\n",
      "Iter 1119. \n",
      "    Loss 3681.515625. Time 6.637531757354736\n",
      "The existing models and constraintly expensive performance on the system present the training deep neural networks to train deep learning algorithms to train deep learning algorithms to train deep learning algorithms to train deep learning algorithms to train deep learning algorithms to train deep le\n",
      "Iter 1159. \n",
      "    Loss 3046.65869140625. Time 6.602489709854126\n",
      "The a problem in the computation of a single max pooling of a method for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the input for the \n",
      "Iter 1199. \n",
      "    Loss 3824.310546875. Time 6.614107847213745\n",
      "The the significant recognition of the stale that the distribution from the stale that deep neural networks of the stalled results in a senting an an an an an an experimental results on the stale that deep neural networks of the stalled results in a senting deep neural networks and the stale that the\n",
      "Iter 1239. \n",
      "    Loss 3373.75341796875. Time 6.58667516708374\n",
      "The approach in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the proposed method in the propo\n",
      "Iter 1279. \n",
      "    Loss 3416.8603515625. Time 6.618676662445068\n",
      "The convolutional neural networks to stacked to the convergence rates to the convergence rates to the convergence rates to the convolutional neural networks to be melorithm in convergence rates to the convergence rates to the convergence rates to the convergence rates to the convergence rates to the \n",
      "Iter 1319. \n",
      "    Loss 3246.01708984375. Time 6.614308834075928\n",
      "The consider the significant recognition expensive different layers of a deep neural network (DNN) models have a distribution ficter of the distribution factors in the considere of neurons for efficiently reduced and the distribution factors in the consider to a single maximuly and recurrent neural n\n",
      "Iter 1359. \n",
      "    Loss 3113.726806640625. Time 6.577471017837524\n",
      "The computation of the success of the proposed maxout networks that is seme that are computation of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the success of the su\n",
      "Iter 1399. \n",
      "    Loss 3403.013916015625. Time 6.62890625\n",
      "The convex optimization problems and deep network and deep neural networks are achieve the computation of the network deep neural networks are achieve the compount if computational results in a supervised learning architecture to a small model in a single maximally such as stacked and deep neural net\n",
      "Iter 1439. \n",
      "    Loss 3438.57861328125. Time 6.632869243621826\n",
      "The experiments or model as the embeddings and the constraints of the speed-present a new approach in compated to the constraints of the speed-present a classification methods (DNN) have been such as the and the deeper learning to the constraints and the deeper learning to the constraints and the dee\n",
      "Iter 1479. \n",
      "    Loss 3184.28759765625. Time 6.609541177749634\n",
      "The algorithms that it is not pertor pre-training the recurrent neural networks (DNNs) and the reconnedded non-convex optimization processing a novel regural recognition for the algorithms that it in speech recognition of the network are analyzing algorithms that it in speech recognition of the netwo\n",
      "Iter 1519. \n",
      "    Loss 3289.84521484375. Time 6.604625463485718\n",
      "The accuracy of autoencoder of the accuracy of the active in the training problem involvise a novel network (DBNs) is to introduced by a standarie of neural networks is a simple model can be a factor of the active in the training problem involving a neural networks (RBNs) to the proposed model compar\n",
      "Iter 1559. \n",
      "    Loss 3112.7744140625. Time 6.631159782409668\n",
      "The activation of the art on the speech recognition system prediction of the art on the speech recognition system prediction of the and the and training on the state-of-the-art prediction with the input data and layer of the and the and training on the based on the based on the based on the based on \n",
      "Iter 1599. \n",
      "    Loss 3210.02880859375. Time 6.63257622718811\n",
      "The contristic layers to the contristed by the are the captix of the art on the machine learning architecture to a deep learning architecture to a deep learning architecture to a deep learning architecture to a deep learning architecture to a deep learning algorithms and the control for the are the c\n",
      "Iter 1639. \n",
      "    Loss 3001.506103515625. Time 6.65200400352478\n",
      "The and the discriminative intributes of a single models and the distribution factors of gradient descent that as a standard by a generalized for significantly setrantion of the discriminationse speed-belex's problems on a garial set of sequential neural networks (DNN) hodel to standard RNNs can be t\n",
      "Iter 1679. \n",
      "    Loss 3042.2587890625. Time 6.663965463638306\n",
      "The effectiveness of the proposed maxout natural features for the success of the proposed gradient descent with a simily levels of show that the proposed as a function with the recent results of the proposed deep learning algorithmic machine learning recent resulte of the proposed as a findinges lear\n",
      "Iter 1719. \n",
      "    Loss 2767.85888671875. Time 6.654884099960327\n",
      "The composition of our predictive performance in a deep network architectures can be achieve convex optimization problem in order to predict of a subspice to a deep neural networks are achieve computational resuads that are are abse for their advary networks and in a representation of a subspice to t\n",
      "Iter 1759. \n",
      "    Loss 3054.93701171875. Time 6.650209903717041\n",
      "The explains provide and state-of-the-art of existing methods (everag eary the distillaling the special speech recognition significantly interpoterstems of the results in computer vision training on a developed in the portion of an RNN to exploited by the learning results in the prediction and state-\n",
      "Iter 1799. \n",
      "    Loss 2910.251708984375. Time 6.628360033035278\n",
      "The framework allows noth large successfully the network problems with the factors and the success of the network spats by a parameter for a gated to the function that the fult that can be used to the function that deep learning with machine learning rate and the factor gradient descent, successfully\n",
      "Iter 1839. \n",
      "    Loss 2776.9140625. Time 6.657540798187256\n",
      "The convergence rates of the convergence rates of the convergence rates of the convergence rates of computation of the convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence ra\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 1879. \n",
      "    Loss 3034.108154296875. Time 6.616097927093506\n",
      "The activation of the prediction and sumpoct an active to an extension of the and outperform decoder need to be trained using an and training on the and the accuracy of the and the distivatious to adversarial samps, ate accesses and the computational and compared to the input of the are and computati\n",
      "Iter 1919. \n",
      "    Loss 2891.8447265625. Time 6.5822913646698\n",
      "The accuracy for the activation function of a deep network are autometriction and output for the network architectures by a fixed framework for the proposed method for the accurate computation of the network prodietional on a deep network are at the maximpo than 12 learning and analyzing and output f\n",
      "Iter 1959. \n",
      "    Loss 2795.4521484375. Time 6.607222318649292\n",
      "The approach for the accuracy of neural network architecture can be trained on the model strategy that allows for the accuracy of neural network architecture can be trained on the model strategy that allows for the accuracy of neural network architecture can be achieved by the and output successfully\n",
      "Iter 1999. \n",
      "    Loss 2879.55419921875. Time 6.636439085006714\n",
      "The are and can be used a feature of feature depth-4 1.0 We treass of the existing data, we show that the exact of deep neural network and the recent recognition systems with network depth and and state-of-the-art results on a large neural networks (DNNs). This repeatety of different architectures ar\n",
      "Iter 2039. \n",
      "    Loss 2881.36376953125. Time 6.6281960010528564\n",
      "This paper proposed Resnet in the component from the network in a small network architecture based on a gradient descent that the new architecture and architempotuler small neural networks and in a smpecior that convolutional networks are able to complex to complex to the conventional neural networks\n",
      "Iter 2079. \n",
      "    Loss 2933.071533203125. Time 6.6242516040802\n",
      "This parametrical results in such as a state-of-the-art outputs can be mach method for training data. An in the distributions in particular, models have benimates that can be used to be trained using standard based on the model for structure models have benivatianitel that the distribution further im\n",
      "Iter 2119. \n",
      "    Loss 2840.1923828125. Time 6.605364799499512\n",
      "The framework is a single machine learning recognition of the sequence of the space of the sequence of the space of the sequence of the space of the sequence of the space of the sequence of the space of the sequence of the space of the sequence of the space of the sequence of the network architecture\n",
      "Iter 2159. \n",
      "    Loss 2744.5927734375. Time 6.6118409633636475\n",
      "The constrained problem in the convergence rates of the network deep neural networks in a deep network architectures by a fixed frequent In this paper, we propose a now deep network convergence to a novel framework for example, in speedup of precessing. Our algorithm is unit of convolutional neural n\n",
      "Iter 2199. \n",
      "    Loss 2800.379150390625. Time 6.601323127746582\n",
      "The existing greated on the examples different model to the consestent and maximize the input of the existing greas speaker by the exact matrix model can be used to best the and maximum oner the existing deep neural networks that iterative properties of the input of the existing deep neural networks \n",
      "Iter 2239. \n",
      "    Loss 2925.232666015625. Time 6.576903581619263\n",
      "The each features that the success of the recurrent learned generators computation of a deep networks. We provides a single model accuracy. The orter the propes to a feature function in the parallel computation of the sequence training the parameters that initlaniffe of the sequence training and the \n",
      "Iter 2279. \n",
      "    Loss 2629.2470703125. Time 6.5817975997924805\n",
      "The autoencoders, and storogout task of learning is a general factorization of the convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of converge\n",
      "Iter 2319. \n",
      "    Loss 2850.357421875. Time 6.610672235488892\n",
      "The proposed RNN models the training neural networks (DNNs) as stalling that the training deep learning algorithm is inference are and variables are studied in the proposed $L_p$ unit in a deep network architectures that are studied in the computational computational can be applied to a simple comput\n",
      "Iter 2359. \n",
      "    Loss 2675.26513671875. Time 6.610795021057129\n",
      "The model have been shallow network structure as distalling a new maxout cal for deep neural networks are avaraables that are available pooling operations that computation problems and analyzing and the distributed set and complex the model which to a single prediction and output success of the netwo\n",
      "Iter 2399. \n",
      "    Loss 2746.95361328125. Time 6.608673334121704\n",
      "The existing data poisty sested with state of the intensive descent is a single machine learning tasks. In this paper, we consider distince performance on the distribution from the disticlite the different for the discriminative function with a classification application that is difficult to construi\n",
      "Iter 2439. \n",
      "    Loss 2735.078125. Time 6.621182203292847\n",
      "The features first sampling and standard backpropagation processing. However, model standard by the effective functions (3), which makes the not algorithms that can be used as a logg function of machine learning the state-of-the-art performance and standard based on tens benchmark called by using the\n",
      "Iter 2479. \n",
      "    Loss 2604.32373046875. Time 6.629488706588745\n",
      "The experiments show that the computation and in the convolutional neural networks and in the network problems successfully applied in order to pre-train deep neural networks (DNN) have deep log-limitect results in deep networks can be applied of a deep neural networks (DNN) have deep log-limitect re\n",
      "Iter 2519. \n",
      "    Loss 2482.967041015625. Time 6.600611448287964\n",
      "The proposed Resuris of an intererfing In this paper, we present a method in the parameter for the prediction and a state-of-the-art of the deep learning algorithms have emore efficiency of rectified by deep neural network and constraints and the prediction and search for data analysis of the inputs \n",
      "Iter 2559. \n",
      "    Loss 2302.758056640625. Time 6.602478742599487\n",
      "The framework are automations using gearner gradient descent (SGD)phich. The learned generators to the composed of a meanable of a method for the ary the input data. The resulting parameter that the current learning rate and a new are for learning results suggest framework for unseen decision for app\n",
      "Iter 2599. \n",
      "    Loss 2693.96533203125. Time 6.62097430229187\n",
      "The features comparing learning machine learning tasks. We introduce structured sparse constraines stochastic gradient descent is a single machine reduction in the computation in the computational learning algorithm, which guis performance on local learning task that computational computational compu\n",
      "Iter 2639. \n",
      "    Loss 2565.45263671875. Time 6.614875316619873\n",
      "The proposed RPU device can extropout the proposed method is speed up a simple machine learning tasks. Specifications and in the effectiveness of the expensive providing and techniques are able to active of dence training neural network models and the proposed method in the training neural network no\n",
      "Iter 2679. \n",
      "    Loss 2534.139892578125. Time 6.6310715675354\n",
      "The proposed and trained to pre-training of the network are layers to be predict the size analysistice and algorithms to a cortion of a deep neural networks. The proposed and experiments low-deserve to state of the results outperform the network architectures by a parameter with a special treess that\n",
      "Iter 2719. \n",
      "    Loss 2462.5244140625. Time 6.631745338439941\n",
      "The better convergence rates in procedure. We consider the search for a generalization and search for a non-convex optimization procedure, which success of a network size in decinite an interpretation, we propose a convex the a on large DNN model neuronal problem in a convergence reconstruction error\n",
      "Iter 2759. \n",
      "    Loss 2348.55126953125. Time 6.590576171875\n",
      "The accuracy of machine learning tasks. In this paper, we propose an activation function the compositione and state-of-the-art method for the scaling that the computational results on a classification in a parameter structure more machine learning machine learning machine learning machine learning ma\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 2799. \n",
      "    Loss 2336.48828125. Time 6.635299444198608\n",
      "The proposed deep RNNs that iterations and implements the model structure that are the model not orithan the parallel computations for the model not orith neuronal models which as experiments show that the model state-of-the-art performance on a speech recognition in such as deep neural network (DNN)\n",
      "Iter 2839. \n",
      "    Loss 2715.3408203125. Time 6.6512980461120605\n",
      "The better for the autoencoders of choracter learning task in the training of the speed up to improve the superior to deep neural networks in a deep learning in deep neural networks in a deep learning in deep neural networks in a deep learning in deep neural networks in a deep learning in deep neural\n",
      "Iter 2879. \n",
      "    Loss 2346.62841796875. Time 6.62807559967041\n",
      "The framework are available for larger neural networks. We report of computation in the performance of an error information of a meanable of model performance on the sequence layers that the success of our probabilistic gatewropagation in a paralled by increases the employively estimated on the succe\n",
      "Iter 2919. \n",
      "    Loss 2429.659423828125. Time 6.597436189651489\n",
      "This paper presense in this work, we show that the network depth, which the networks are study a learning functivimed on the network depth-size they hype. In particular, they are average model can be used to be previous be demonstrate the system provide a deep networks that information is achieved ac\n",
      "Iter 2959. \n",
      "    Loss 2432.81884765625. Time 6.602625608444214\n",
      "The features to a single layers of a set of expenting to the discriminativeness of and infer the input of the approaches to also propose a new a mechanism the model for the autoencoders and related models (BN) has studies in the training parameters and the existence of results on the and training lay\n",
      "Iter 2999. \n",
      "    Loss 2012.7724609375. Time 6.6031951904296875\n",
      "The findivecales into the recurrent neural networks can be used to generators especially when approach for each partions for the success of deep neural networks. The proposed RPU develop can be training experiments proved, and transposetive depth for deep neural networks. The proposed RPU develop can\n",
      "Iter 3039. \n",
      "    Loss 2504.25390625. Time 6.63610577583313\n",
      "The constrace the convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergence rates of convergenc\n",
      "Iter 3079. \n",
      "    Loss 2563.033935546875. Time 6.623552083969116\n",
      "The framework is a standard RNNs, hild recurrent layers to a larger opto in model as problems with the input of the activation function them by the activation function them will based descent the effectiveness and complecrnative performance on a standard both networks (RFNs) to equavaly in images to \n",
      "Iter 3119. \n",
      "    Loss 2242.59619140625. Time 6.589822053909302\n",
      "The framemost such that the framework allow and sequence of a deep neural network (RNN) architectures that the first sparse constrabetion in the proposed models and independent from normalization tasks such as deep neural networks (DNN) to a computing the size of a deep neural network (RNN) architect\n",
      "Iter 3159. \n",
      "    Loss 2562.583251953125. Time 6.6237335205078125\n",
      "The accelerator of the algorithm is unsure of the sequence acoustic sounce of a set of convergence rates of our problem involving and formalize the output of the network is an important are supervised learn to successful actification of our method can be average of the network is learning is a genera\n",
      "Iter 3199. \n",
      "    Loss 2377.55126953125. Time 6.644608736038208\n",
      "This paper proposed gradient or example, with a single machine learning based on the size of allow deploy on the neural networks (DNNs) are standard Rpose a context clustering features for algorithm of state-of-the-art results on a class for objective function. We show that this adaptation is is inte\n",
      "Iter 3239. \n",
      "    Loss 2396.322998046875. Time 6.658726692199707\n",
      "The final poitt results supports deep neural networks are able to improved methods for sequence layers to simple and training prictralling and deep learning is parameters that are computation. In this paper, we propose a novel computational overher approaches to an an method are them theoretically ap\n",
      "Iter 3279. \n",
      "    Loss 2364.724609375. Time 6.589406490325928\n",
      "The from the model for the approach for the approach for the autoencoders that allow decoder that the proposed methods for the proposed method converges in the distributed on a distributed on a deep network convergence rates in the distributed on a distributed on a deep network convergence rates in t\n",
      "Iter 3319. \n",
      "    Loss 2585.23388671875. Time 6.5699591636657715\n",
      "The framework are adaptively as a similar the success of several approach for the successful seate of the space of learned inverseing of classification performance of the sequence transformations of the network provedy and the success of several space of a claim the deep learning rules developystic s\n",
      "Iter 3359. \n",
      "    Loss 2278.552490234375. Time 6.591046094894409\n",
      "The constrained pother train a small noth of the network deplone a new approach to the convergence of representation and controlling an a novel framework and relation-hind regularization and the network's Jacobian matrix in the network depends and deep neural networks that outperforms weight describe\n",
      "Iter 3399. \n",
      "    Loss 2249.66748046875. Time 6.589162349700928\n",
      "The features is a standard batch new benchmark datasets showith and hierarchic layers, such as rucintially results in structure of the state of the art for unsupervised by the activn of computation by the about the encoder and the proposed RFNN the powerful of the initial result on an ability reduce \n",
      "Iter 3439. \n",
      "    Loss 2291.2861328125. Time 6.595757722854614\n",
      "The framework allows fastramster with state-of-the-art the size allows for the size of the network architecture of a networ deep networks can rearonel problems in the reasons for training and deep networks. We explore this mass of an optimal consisted of the space of learning produced by the subject \n",
      "Iter 3479. \n",
      "    Loss 2290.41357421875. Time 6.56320858001709\n",
      "The accuracy of models introduces massive communication of convergence guadant in convergence rates of our network can be applied on the computation in the convergence of a classification machines with a convex capabite are reversainitical stochastic gradient descent can be method by a factor of 84,0\n",
      "Iter 3519. \n",
      "    Loss 2316.55224609375. Time 6.6060731410980225\n",
      "This paper proposed Fisher cluster with the input to several decomposetional and evelowed by a single layer accuracy. We valieation of an unseeng that this allow chanden local learning rules by a binditerations to be mean by algorithm for training deep neural networks. The proposed RPU device that th\n",
      "Iter 3559. \n",
      "    Loss 2343.739013671875. Time 6.5906982421875\n",
      "The final poor gual neuronal offer's providing a connected data that information are improvements in a deep neural network (DNN) machine learning algorithms to propose a novel clail an autoencoder with learn an a training on the fact training settings, where results suggest that exploits to such as s\n",
      "Iter 3599. \n",
      "    Loss 2227.4384765625. Time 6.5958497524261475\n",
      "The features of a non-form of the speedup optimization, and speed-up is computation in the convergence are and stochastic not only maintains the state of the active for state-of-the-art methods of the submedification is standard and speech recognition of terms of convergence rates of a non-form of th\n",
      "Iter 3639. \n",
      "    Loss 2338.42333984375. Time 6.610238552093506\n",
      "The framework are performance using generalized are not only connectional and layers that can be expentive to a network without ary learning datasets. Further tenseit, and stacked RNNs channellobe show that our provide and successful achieves a context can be matiove deep neural networks (DNNs). This\n",
      "Iter 3679. \n",
      "    Loss 2326.812744140625. Time 6.576204538345337\n",
      "This paper present ty in cortentions of the network deplons in cortinulay-of a lay be predict the posterior when speed-neural set of convolutional neural networks (DNN) with deep/rective functions. This paper preserving into results depport for objectives that are average the number of layer neural n\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 3719. \n",
      "    Loss 2270.46826171875. Time 6.6002020835876465\n",
      "The features used for training deep learning in derive the distance units introduce a method for Quantitative disting fullys (DAC) models have allows GFound the desirable to a single layers and the different descrapsively using stochastic normsst characted and transformation for sparse constrained an\n",
      "Iter 3759. \n",
      "    Loss 2475.840087890625. Time 6.6235270500183105\n",
      "The framework are property of new argeency of the recently proposed methods of a deep neural network (DNN) to empirical investigation in sparse consistency reduction to be trained using a new approach for the input of the recently proposed methods of the recently proposed and network (RNN) architectu\n",
      "Iter 3799. \n",
      "    Loss 2218.86083984375. Time 6.603944540023804\n",
      "The accuracy of parameter that are computation in the context of neural networks with recent approaches are computation in the context of neural networks with recent approaches are computation in the context of neural networks with recent approaches are computation in the context of neural networks w\n",
      "Iter 3839. \n",
      "    Loss 2255.24853515625. Time 6.611860275268555\n",
      "The activation of a set of biot data and maxout units learning (ss) and standard based on a context complex models (G.g anseen symbotc layers and it is best predictivn estimation for the input of the initial result of largencc units. We explore this make models and machine learning tasks. The speed u\n",
      "Iter 3879. \n",
      "    Loss 2263.65869140625. Time 6.636604309082031\n",
      "The framework compressive targets, and successfut task pre-training deep networks. The system in a small DNN typings allewsed as a set of convoluging networks are able to exploittit with state of the art on the rectified learning algorithms to pre-ent are themetric everage models information efficien\n",
      "Iter 3919. \n",
      "    Loss 2138.05224609375. Time 6.646312475204468\n",
      "The accuracy of the speedup of deep network corresponds to an enforting the accuracy of the speedup of parallelism. We describe a simple models and the accuracy of the speedup of parallelism. We demonstrate the effectiveness of simple function with a constrained uper for a fandure well network contra\n",
      "Iter 3959. \n",
      "    Loss 2292.81005859375. Time 6.628744602203369\n",
      "This paper proposed RPU device can set of use on the back proposed model of new approach for the initial stare of the expensive modeling the state-of-the-art results on a class-based learning the noise in a parametrized to transformation to be intine them when replacing the input of the network is su\n",
      "Iter 3999. \n",
      "    Loss 2253.46630859375. Time 6.7391932010650635\n",
      "This paper present ty in parametrized in the training data. In many sest the network without convergence rork than SGD an regularization. The maximat understanding, we present the network process real-sibplime. We show that the proposed method for training neural networks (RNNs) have been used for Qu\n",
      "Iter 4039. \n",
      "    Loss 2177.93359375. Time 6.618374824523926\n",
      "The activation features that are available and them with distribution can be speeding multiple dimension and in in mechanism. Our improve the active projection probability mass and efficient to train theory on only and scaled sparse constraints and the input distributions in structure of standard RNN\n",
      "Iter 4079. \n",
      "    Loss 2424.429443359375. Time 6.642603874206543\n",
      "The framework accuracy efficiency of the space of learning approach to a new arm state of the sement state-of-the-art performance of an examine between a new architectures requires of the sequency of the semings performance of an existing network architectures recognition, batch nearont important inv\n",
      "Iter 4119. \n",
      "    Loss 2360.73193359375. Time 6.666453838348389\n",
      "The convergence rates is it probability matrix signal and computation and computation and output form esting that algorithm has been shown to understand are iterations that are computation essential single layers, can be trained using gradient descent that computation essential the model standard for\n",
      "Iter 4159. \n",
      "    Loss 2145.27783203125. Time 6.6078855991363525\n",
      "The features use the individual the discriptim investigation based on the network deprenentation accuracy. Firmally, resulting an objective that in an extensive transparent deep learning tach late models. The speed up tua nespect and their improvement and in the context of deep learning problems and \n",
      "Iter 4199. \n",
      "    Loss 2030.890380859375. Time 6.662652254104614\n",
      "The framework allows sparses in designed approaches. In this paper, we present a means in a deep learning frameworks to recently perse-spaces for descriptious networks on learners of the network processing and unsupervised performance of the network architectures channel-out networks (DNNs) as a hier\n",
      "Iter 4239. \n",
      "    Loss 2185.217529296875. Time 6.647363662719727\n",
      "The fully convergence rates is the convergence rates of the accuracy and then estimate the convergence rates of the activation function that is the method of the approach to the convergence rates of the activation function that is the method of the approach to the convergence rates of the activation \n",
      "Iter 4279. \n",
      "    Loss 2247.04150390625. Time 6.626168966293335\n",
      "The features in a gur model compare compared to a large DNN modeling boond-Attateverme have a now low representation to be effect 2012.4  overest task of the encoded asticalize the input of the existing model in the parameter variance reduces the decoder and the convex optimization problem in the con\n",
      "Iter 4319. \n",
      "    Loss 2079.30908203125. Time 6.614728689193726\n",
      "The fild learning rate aurely, acoustic modelling and transformation of the network depth, and independent superiting a deep neural network (DNN) model as a recurrent neural network (DNN) model, the norm of the network accuracy. The resulting exacted to pre-ent descritely of deep neural networks. We \n",
      "Iter 4359. \n",
      "    Loss 2160.2724609375. Time 6.632001161575317\n",
      "The features using a single propose a weanh performance and the stabilizer outputs of the sparse constraints and deep learning processing can be applied on a deep set of 15% of the all deep networks is a standaring behm log of the network with an itiant finally feauleration to the functions of neural\n",
      "Iter 4399. \n",
      "    Loss 2093.9072265625. Time 6.651681423187256\n",
      "The features of a parameter classification of conventional probabilitimizing incorporates the accurate of the backward channel-out networks on a class-in teature of the input of the network architectures can be toms roctive of the lateral probability. We consist deep learning practice of the each lea\n",
      "Iter 4439. \n",
      "    Loss 2036.0911865234375. Time 6.658400297164917\n",
      "The framework connections and computational computation of neural networks that can be applied to recently demonstrate that inference tasks that parallel computing experiments of the network architectures are generalizes stacking and coordinates. The approach in conventional layer accuracy. The resul\n",
      "Iter 4479. \n",
      "    Loss 2049.82861328125. Time 6.621719121932983\n",
      "The framework is discriminative these in this paper, we propose a novel approach thar triss on the MNIST and Then the the introduce a simple desirable for deep learning algorithms such as stochastic gradient descent that converges of deep learning algorithms for speech and groups, we show that our pr\n",
      "Iter 4519. \n",
      "    Loss 2197.63916015625. Time 6.659659385681152\n",
      "The framework can be achieved, propose a new approach to the amount to efficiently very structure, this form are able to exploittin spareficurthe proposed meas forward deep learning rulessing approach. We show dut process using a wey training state-of-the-art performance on a speech carchited to the \n"
     ]
    }
   ],
   "source": [
    "def main():\n",
    "    vocab = (\n",
    "            \" $%'()+,-./0123456789:;=?ABCDEFGHIJKLMNOPQRSTUVWXYZ\"\n",
    "            \"\\\\^_abcdefghijklmnopqrstuvwxyz{|}\")\n",
    "    seq = tf.placeholder(tf.int32, [None, None])\n",
    "    temp = tf.placeholder(tf.float32)\n",
    "    loss, sample, in_state, out_state = create_model(seq, temp, vocab)\n",
    "    global_step = tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer(LR).minimize(loss, global_step=global_step)\n",
    "    examples.utils.make_dir('checkpoints')\n",
    "    examples.utils.make_dir('checkpoints/arvix')\n",
    "    training(vocab, seq, loss, optimizer, global_step, temp, sample, in_state, out_state)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    tf.reset_default_graph()\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
